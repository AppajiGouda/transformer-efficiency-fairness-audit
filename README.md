# transformer-efficiency-fairness-audit
A comparative study benchmarking inference latency of DistilBERT vs. BERT (2.46x speedup) and quantifying sociopolitical biases in pre-trained embeddings using Masked Language Modeling.
